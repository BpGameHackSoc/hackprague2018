{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "import numpy as np \n",
    "import scipy.misc\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "file = 'data/fer2013/fer2013.csv'\n",
    "output = 'data/fer2013/extracted'\n",
    "\n",
    "w, h = 48, 48\n",
    "image = np.zeros((h, w), dtype=np.uint8)\n",
    "id = 1\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "# with open(file, 'r') as csvfile:\n",
    "#     datareader = csv.reader(csvfile, delimiter =',')\n",
    "# #     print headers \n",
    "#     for i,row in enumerate(datareader):  \n",
    "#         if i==0:\n",
    "#             continue\n",
    "#         emotion = row[0]\n",
    "#         pixels = list(map(int, row[1].split()))\n",
    "# #         print(row)\n",
    "#         usage = row[2]\n",
    "#         #print emotion, type(pixels[0]), usage\n",
    "#         pixels_array = np.asarray(pixels)\n",
    "\n",
    "#         image = pixels_array.reshape(w, h)\n",
    "#         #print image.shape\n",
    "\n",
    "#         stacked_image = np.dstack((image,) * 3)\n",
    "#         #print stacked_image.shape\n",
    "\n",
    "\n",
    "#         image_folder = os.path.join(output, usage)\n",
    "#         if not os.path.exists(image_folder):\n",
    "#             os.makedirs(image_folder)\n",
    "#         image_file =  os.path.join(image_folder , str(id) + '.jpg')\n",
    "#         scipy.misc.imsave(image_file, stacked_image)\n",
    "#         id += 1 \n",
    "#         if id % 100 == 0:\n",
    "#             print('Processed {} images'.format(id))\n",
    "\n",
    "# print(\"Finished processing {} images\".format(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "dataset = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, dirname in enumerate(dirnames):\n",
    "#     filenames =[ os.path.join(dirname,filename) for filename in os.listdir(dirname)]\n",
    "#     img=mpimg.imread(np.random.choice(filenames))\n",
    "#     plt.subplot(len(dirnames) / columns + 1, columns, i + 1)\n",
    "#     plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>85 84 90 121 101 102 133 153 153 169 177 189 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>255 254 255 254 254 179 122 107 95 124 149 150...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
       "5        2  55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...  Training\n",
       "6        4  20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...  Training\n",
       "7        3  77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...  Training\n",
       "8        3  85 84 90 121 101 102 133 153 153 169 177 189 1...  Training\n",
       "9        2  255 254 255 254 254 179 122 107 95 124 149 150...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Training', 'PublicTest', 'PrivateTest'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.Usage.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3589, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3589, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training = dataset[dataset['Usage']=='Training']\n",
    "validation = dataset[dataset['Usage']=='PublicTest']\n",
    "test = dataset[dataset['Usage']=='PrivateTest']\n",
    "training.shape\n",
    "validation.shape\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_common = set(training.pixels) & set(validation.pixels)\n",
    "train_test_common = set(training.pixels) & set(test.pixels)\n",
    "len(train_val_common)\n",
    "len(train_test_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = validation[~validation['pixels'].isin(train_val_common)]\n",
    "test = test[~test['pixels'].isin(train_test_common)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixstr2numpy(pixstr):\n",
    "    pixels = np.array(pixstr.split())\n",
    "    pixels = pixels.reshape(w, h)\n",
    "    return pixels.astype(np.int32)\n",
    "\n",
    "def to3D(array):\n",
    "    size = array.shape[0]\n",
    "    X_t = np.zeros((size,48,48))\n",
    "    for i,x in enumerate(array):\n",
    "        X_t[i,:,:] = x\n",
    "    X_t = X_t.astype(np.int32)\n",
    "    return X_t\n",
    "\n",
    "def X_Y_split(df):\n",
    "    X_train,Y_train = df['pixels'].apply(pixstr2numpy),df['emotion'].astype(np.int32)\n",
    "    X_train = np.array(X_train)\n",
    "    X_train = to3D(X_train)\n",
    "    Y_train  = Y_train.values\n",
    "    return X_train,Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train = training['pixels'].apply(pixstr2numpy),training['emotion'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train = X_Y_split(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation,Y_validation = X_Y_split(validation)\n",
    "X_test,Y_test = X_Y_split(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 3, 48, 48)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([X_train, X_train, X_train])\n",
    "X_train =X_train.swapaxes(0, 1)\n",
    "X_validation = np.array([X_validation, X_validation, X_validation])\n",
    "X_validation = X_validation.swapaxes(0,1)\n",
    "X_test = np.array([X_test, X_test, X_test])\n",
    "X_test = X_test.swapaxes(0,1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_obtain_input_shape() got an unexpected keyword argument 'include_top'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-8c37d5fe44e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMobileNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshallow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-5b81e712d25a>\u001b[0m in \u001b[0;36mMobileNet\u001b[0;34m(input_tensor, input_shape, alpha, shallow, classes, include_top)\u001b[0m\n\u001b[1;32m     38\u001b[0m                                       \u001b[0mmin_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                                       include_top=include_top)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _obtain_input_shape() got an unexpected keyword argument 'include_top'"
     ]
    }
   ],
   "source": [
    "MobileNet(input_shape=(3,48,48), alpha=1, shallow=False, classes=7, include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google MobileNet model for Keras.\\n# Reference:\\n- [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/pdf/1704.04861.pdf)\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Convolution2D, \\\n",
    "    GlobalAveragePooling2D, Dense, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import get_source_inputs\n",
    "\n",
    "'''Google MobileNet model for Keras.\n",
    "# Reference:\n",
    "- [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/pdf/1704.04861.pdf)\n",
    "'''\n",
    "\n",
    "def MobileNet(input_tensor=None, input_shape=None, alpha=1, shallow=False, classes=1000, include_top=True):\n",
    "    \"\"\"Instantiates the MobileNet.Network has two hyper-parameters\n",
    "        which are the width of network (controlled by alpha)\n",
    "        and input size.\n",
    "        \n",
    "        # Arguments\n",
    "            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "                to use as image input for the model.\n",
    "            input_shape: optional shape tuple, only to be specified\n",
    "                if `include_top` is False (otherwise the input shape\n",
    "                has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "                or `(3, 224, 244)` (with `channels_first` data format).\n",
    "                It should have exactly 3 inputs channels,\n",
    "                and width and height should be no smaller than 96.\n",
    "                E.g. `(200, 200, 3)` would be one valid value.\n",
    "            alpha: optional parameter of the network to change the \n",
    "                width of model.\n",
    "            shallow: optional parameter for making network smaller.\n",
    "            classes: optional number of classes to classify images\n",
    "                into.\n",
    "        # Returns\n",
    "            A Keras model instance.\n",
    "        \"\"\"\n",
    "\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=96,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      include_top=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    x = Convolution2D(int(32 * alpha), (3, 3), strides=(2, 2), padding='same', use_bias=False)(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = DepthwiseConvolution2D(int(32 * alpha), (3, 3), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(int(64 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = DepthwiseConvolution2D(int(64 * alpha), (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(int(128 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = DepthwiseConvolution2D(int(128 * alpha), (3, 3), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(int(128 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = DepthwiseConvolution2D(int(128 * alpha), (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(int(256 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = DepthwiseConvolution2D(int(256 * alpha), (3, 3), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(int(256 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = DepthwiseConvolution2D(int(256 * alpha), (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(int(512 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if not shallow:\n",
    "        for _ in range(5):\n",
    "            x = DepthwiseConvolution2D(int(512 * alpha), (3, 3), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            x = Convolution2D(int(512 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "\n",
    "    x = DepthwiseConvolution2D(int(512 * alpha), (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(int(1024 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = DepthwiseConvolution2D(int(1024 * alpha), (3, 3), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(int(1024 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    out = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    model = Model(inputs, out, name='mobilenet')\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.backend import image_data_format\n",
    "from keras.backend.tensorflow_backend import _preprocess_conv2d_input, _preprocess_padding\n",
    "from keras.engine.topology import InputSpec\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D\n",
    "from keras.legacy.interfaces import conv2d_args_preprocessor, generate_legacy_interface\n",
    "from keras.utils import conv_utils\n",
    "\n",
    "# This code mostly is taken form Keras: Separable Convolution Layer source code and changed according to needs.\n",
    "\n",
    "\n",
    "def depthwise_conv2d_args_preprocessor(args, kwargs):\n",
    "    converted = []\n",
    "    if 'init' in kwargs:\n",
    "        init = kwargs.pop('init')\n",
    "        kwargs['depthwise_initializer'] = init\n",
    "        converted.append(('init', 'depthwise_initializer'))\n",
    "    args, kwargs, _converted = conv2d_args_preprocessor(args, kwargs)\n",
    "    return args, kwargs, converted + _converted\n",
    "\n",
    "legacy_depthwise_conv2d_support = generate_legacy_interface(\n",
    "    allowed_positional_args=['filters', 'kernel_size'],\n",
    "    conversions=[('nb_filter', 'filters'),\n",
    "                 ('subsample', 'strides'),\n",
    "                 ('border_mode', 'padding'),\n",
    "                 ('dim_ordering', 'data_format'),\n",
    "                 ('b_regularizer', 'bias_regularizer'),\n",
    "                 ('b_constraint', 'bias_constraint'),\n",
    "                 ('bias', 'use_bias')],\n",
    "    value_conversions={'dim_ordering': {'tf': 'channels_last',\n",
    "                                        'th': 'channels_first',\n",
    "                                        'default': None}},\n",
    "    preprocessor=depthwise_conv2d_args_preprocessor)\n",
    "\n",
    "\n",
    "class DepthwiseConv2D(Conv2D):\n",
    "\n",
    "    @legacy_depthwise_conv2d_support\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 strides=(1, 1),\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 depth_multiplier=1,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 depthwise_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 depthwise_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 depthwise_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(DepthwiseConv2D, self).__init__(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs)\n",
    "\n",
    "        self.depth_multiplier = depth_multiplier\n",
    "        self.depthwise_initializer = initializers.get(depthwise_initializer)\n",
    "        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n",
    "        self.depthwise_constraint = constraints.get(depthwise_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if len(input_shape) < 4:\n",
    "            raise ValueError('Inputs to `SeparableConv2D` should have rank 4. '\n",
    "                             'Received input shape:', str(input_shape))\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = 3\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs to '\n",
    "                             '`SeparableConv2D` '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = int(input_shape[channel_axis])\n",
    "        depthwise_kernel_shape = (self.kernel_size[0],\n",
    "                                  self.kernel_size[1],\n",
    "                                  input_dim,\n",
    "                                  self.depth_multiplier)\n",
    "\n",
    "        self.depthwise_kernel = self.add_weight(\n",
    "            shape=depthwise_kernel_shape,\n",
    "            initializer=self.depthwise_initializer,\n",
    "            name='depthwise_kernel',\n",
    "            regularizer=self.depthwise_regularizer,\n",
    "            constraint=self.depthwise_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.filters,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        if self.data_format is None:\n",
    "            data_format = image_data_format()\n",
    "        if self.data_format not in {'channels_first', 'channels_last'}:\n",
    "            raise ValueError('Unknown data_format ' + str(data_format))\n",
    "\n",
    "        x = _preprocess_conv2d_input(inputs, self.data_format)\n",
    "        padding = _preprocess_padding(self.padding)\n",
    "        strides = (1,) + self.strides + (1,)\n",
    "\n",
    "        outputs = tf.nn.depthwise_conv2d(inputs, self.depthwise_kernel,\n",
    "                                         strides=strides,\n",
    "                                         padding=padding,\n",
    "                                         rate=self.dilation_rate)\n",
    "\n",
    "        if self.bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            rows = input_shape[2]\n",
    "            cols = input_shape[3]\n",
    "        elif self.data_format == 'channels_last':\n",
    "            rows = input_shape[1]\n",
    "            cols = input_shape[2]\n",
    "\n",
    "        rows = conv_utils.conv_output_length(rows, self.kernel_size[0],\n",
    "                                             self.padding,\n",
    "                                             self.strides[0])\n",
    "        cols = conv_utils.conv_output_length(cols, self.kernel_size[1],\n",
    "                                             self.padding,\n",
    "                                             self.strides[1])\n",
    "        if self.data_format == 'channels_first':\n",
    "            return (input_shape[0], self.filters, rows, cols)\n",
    "        elif self.data_format == 'channels_last':\n",
    "            return (input_shape[0], rows, cols, self.filters)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DepthwiseConv2D, self).get_config()\n",
    "        config.pop('kernel_initializer')\n",
    "        config.pop('kernel_regularizer')\n",
    "        config.pop('kernel_constraint')\n",
    "        config['depth_multiplier'] = self.depth_multiplier\n",
    "        config['depthwise_initializer'] = initializers.serialize(self.depthwise_initializer)\n",
    "        config['depthwise_regularizer'] = regularizers.serialize(self.depthwise_regularizer)\n",
    "        config['depthwise_constraint'] = constraints.serialize(self.depthwise_constraint)\n",
    "        return config\n",
    "\n",
    "DepthwiseConvolution2D = DepthwiseConv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pixels = np.array(dataset.iloc[0,1].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = pixels.astype(np.int32)\n",
    "pixels = pixels.reshape(w, h)\n",
    "im = Image.fromarray(pixels,mode='I') # np.asarray to convert back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp(pixels):\n",
    "    pixels = np.array(pixels.split())\n",
    "    pixels = pixels.astype(np.int32)\n",
    "    pixels = pixels.reshape(w, h)\n",
    "    im = Image.fromarray(pixels,mode='I') \n",
    "    imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis(array):\n",
    "    im = Image.fromarray(array,mode='I') \n",
    "    imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd687053860>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/formatters.py:364: FormatterWarning: image/png formatter returned invalid type <class 'numpy.ndarray'> (expected (<class 'bytes'>, <class 'str'>)) for object: <IPython.core.display.Image object>\n",
      "  FormatterWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display,Image as jupI\n",
    "display(jupI(pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
